{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# COGS 188 - Project Proposal\n",
        "## Using Reinforcement Learning & OpenAI Gym to Build a Playable Pokemon Battle AI Agent\n",
        "\n",
        "### Names\n",
        "- Karun Mokha\n",
        "- Ke Zhang\n",
        "- Pranjli Khana"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Abstract\n",
        "Our project aims to develop an AI agent capable of competitively playing Pokemon battles at varying levels of difficulty. We will create a battle environment within OpenAI Gym, leveraging existing open source battle simulators so we focus on developing and tuning our AI and do not get bogged down by the intricacies of the game mechanics. In the context of Pokemon battles, using AI to simulate numerous possible battle sequences can help the AI learn and determine the best move and strategy to maximize the odds of winning. Our idea is to use a technique like RL, Monte Carlo Search Tree, or Proximal Policy Optimization, where the AI evaluates potential outcomes based on each turn during battle and refines its strategy based on simulation. Essentially choosing the move that moves towards the most winning-likely scenarios based on the game state, feature engineering, and reward shaping in which we implement. Performance will be measured through the AI’s win rate in a fixed number of test battles against scripted and human opponents, the number of moves chosen by the AI which are super effective against the opponent, and adaptability-measured by the change in win rate against different scripted opponents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Background\n",
        "Since Pokémon’s video game debut Pokémon Red, Blue, and Yellow in 1996 on the Game Boy, there have existed artificially intelligent characters that users encountered and battled against on their adventure. These in-game AI opponents followed simple, rule-based strategies to select moves and switch Pokémon, providing a baseline level of challenge for players. However, these early AI implementations were limited in their strategic depth and adaptability.
The field of artificial intelligence for game playing has seen significant advancements over the years, with notable successes in complex games such as chess, Go, and Dota 2. DeepMind's AlphaGo demonstrated the power of reinforcement learning and Monte Carlo Tree Search (MCTS) in mastering the game of Go, a game with a vast state space and deep strategic complexity. AlphaGo's success was achieved by combining deep neural networks with MCTS, allowing the AI to evaluate and plan moves by simulating future states and backpropagating results through a search tree (Silver, D., et al, 2016). 

OpenAI developed AI agents capable of playing the multiplayer online battle arena game Dota 2 at a competitive level. These agents used a combination of reinforcement learning techniques to learn complex strategies and adapt to dynamic game environments (OpenAI, 2018). 

Applying these advanced AI techniques to Pokémon battles introduces unique challenges due to the game's specific mechanics, such as type advantages, move sets, and turn-based strategies. Recent research in this area has explored various approaches to developing competitive AI for Pokémon battles. 

For example, a project by Stanford University implemented a state-based AI algorithm for Pokémon battles using minimax and Monte Carlo methods.The Stanford project utilized Monte Carlo Tree Search (MCTS) to simulate numerous future battle sequences and select optimal moves based on the outcomes of these simulations. MCTS, combined with an evaluation function that considers the health of Pokémon, allowed the AI to make informed decisions and adapt its strategy based on the game state (Burkett, Tibrewala, Zhang, 2021). 

Additionally, statistical data on Generation 1 Pokémon, available from sources like Kaggle, provides a comprehensive foundation for modeling the game accurately. This data includes critical information about Pokémon stats, moves, and type interactions, which are essential for developing a realistic and competitive AI.

We aim to build upon this prior work by developing an AI agent that leverages MCTS and reinforcement learning that not only excels in strategic decision-making but also adapts to different opponents, providing a robust and challenging experience for players."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Problem Statement\n",
        "The primary problem we aim to solve is developing an AI agent capable of competitively playing Pokémon battles at varying levels of difficulty. This problem involves creating an AI that can make strategic decisions during battles to maximize its chances of winning.

Our problem has multiple ML-relevant solutions, including Reinforcement Learning, Monte Carlo Search Tree, and Proximal Policy Optimization. The problem, and battles are quantifiable, as they can be expressed entirely in mathematical or logical form, through State Representation of the battle (stats, HP, condition of the pokemon), the Action Space (all possible moves the AI can take), and through its reward function. Our problem can be measured through win rate, move effectiveness, and adaptability against different opponents. It is also replicable as the problem occurs more than one, and each battle can be simulated with the same or different pokemon to ensure reliability and validity of results. It can also be trained repeatedly due to our use of OpenAI gym, making our experiments and results more reproducible"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data\n",
        "https://www.kaggle.com/datasets/tuannguyenvananh/pokemon-dataset-with-team-combat
https://www.kaggle.com/datasets/abcsds/pokemon

This dataset has a total of 4 csv files:
2 files (combat.csv and pokemon.csv) 
The other 2 files are a 6 vs 6 team match.

Dataset Links:
Pokemon Dataset with Team Combat on Kaggle: https://www.kaggle.com/datasets/tuannguyenvananh/pokemon-dataset-with-team-combat
Pokemon with Stats on Kaggle: https://www.kaggle.com/datasets/abcsds/pokemon
Description of the Datasets:
Pokemon Dataset with Team Combat: This dataset consists of 4 CSV files, providing comprehensive information on individual Pokémon statistics and their performance in both 1 vs 1 and 6 vs 6 team combats.
Size of the Dataset:
pokemon.csv: Contains 800 observations (each representing a Pokémon) with 13 variables (such as Name, Type, Total Stats, HP, Attack, Defense, Speed, etc.).
combat.csv: Contains 50,000 observations, detailing 1 vs 1 combat outcomes between Pokémon.
team_pokemon.csv and team_combat.csv: These files include data for 6 vs 6 team battles, providing insights into team dynamics and battle outcomes.
Observations:
An observation in pokemon.csv represents an individual Pokémon, including its name, type, and various statistics.
An observation in combat.csv represents the outcome of a single battle between two Pokémon, indicating which Pokémon won.
Observations in team_pokemon.csv and team_combat.csv represent team configurations and their combat outcomes.
Critical Variables:
Name: The name of the Pokémon.
Type: The elemental type(s) of the Pokémon (e.g., Fire, Water, Grass).
Total: The total stat value of the Pokémon.
HP: Hit Points, the health of the Pokémon.
Attack: The physical attack stat.
Defense: The physical defense stat.
Speed: The speed stat, determining move order in battles.
Combat Result: Indicates the winner of a battle in combat.csv.
Special Handling and Cleaning:
Data Cleaning: Ensure that all Pokémon entries are consistent and free of duplicates. Verify the accuracy of battle outcomes and correct any discrepancies.
Feature Engineering: Derive additional features from the existing data, such as type effectiveness, status conditions, and combined team stats for team battles.
Normalization: Normalize numerical features to ensure they are on comparable scales, which is crucial for the performance of machine learning models.
Data Transformation:
State Representation: Convert Pokémon stats and battle conditions into a structured format that can be used as input for the AI agent. This includes encoding type matchups and status effects.
Action Space Representation: Define the possible actions the AI can take in a structured manner, such as available moves or switching Pokémon.
Reward Function Design: Establish a reward system that incentivizes effective strategies, such as rewarding super effective moves and penalizing ineffective ones.
Pokemon with Stats: This dataset includes 721 Pokémon, detailing their basic stats and types. It is useful for understanding how Pokémon attributes affect their battle performance.
Size of the Dataset:
Contains 721 observations (each representing a Pokémon) with variables such as ID, Name, Type 1, Type 2, Total, HP, Attack, Defense, Special Attack, Special Defense, and Speed.
Observations:
An observation consists of the Pokémon's ID, name, primary and secondary types, and their base stats.
Critical Variables:
ID: Unique identifier for each Pokémon.
Name: The name of the Pokémon.
Type 1: The primary elemental type of the Pokémon.
Type 2: The secondary elemental type, if any.
Total: The sum of all base stats.
HP: Hit Points, or health, defining how much damage a Pokémon can withstand.
Attack: Base modifier for physical attacks.
Defense: Base damage resistance against physical attacks.
Special Attack: Base modifier for special attacks.
Special Defense: Base damage resistance against special attacks.
Speed: Determines which Pokémon attacks first in a round.
Special Handling and Cleaning:
Data Cleaning: Ensure data consistency, especially in Pokémon names and types.
Feature Engineering: Create additional features such as type effectiveness.
Normalization: Normalize base stat values for consistent comparison.
By leveraging these datasets, we can effectively train our AI agent to simulate and learn from Pokémon battles, enabling it to make strategic decisions and improve its performance over time.",
        "- **Pokemon Dataset with Team Combat**: Contains individual Pokémon statistics and their performance in team combats.\n",
        "- **Pokemon with Stats**: Provides comprehensive data on Pokémon attributes affecting their performance in battles. \n",
        "Data preparation involves cleaning, normalization, and feature engineering to ensure the AI can effectively learn and make decisions based on this data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Proposed Solution\n",
        "Our solution involves developing an AI agent using a combination of Monte Carlo Tree Search (MCTS) and Reinforcement Learning (RL) to effectively play Pokémon battles by making strategic decisions to maximize its chances of winning.

Algorithmic Description:
State Representation:
Each game state will include the current Pokémon's stats (HP, Attack, Defense, Speed, etc.), status conditions (e.g., Burned, Paralyzed), move details (type, power, accuracy), and type matchups against the opponent's Pokémon.
Action Space:
The AI can choose from all available moves for the current Pokémon or switch to another Pokémon. These actions are represented in a discrete action space.
Reward Function:
The reward function will be designed to encourage effective gameplay. Rewards will be given for actions that lead to favorable outcomes, such as dealing super effective damage or causing a status condition. Penalties will be applied for ineffective actions, such as using moves that are not very effective
Monte Carlo Search Tree:
MCTS will be used to explore the potential future states of the game by simulating multiple possible sequences of actions. At each turn, the AI will use MCTS to predict the outcomes of different move sequences, choosing the move that leads to the most favorable predicted outcome. MCTS will help the AI consider the long-term effects of its actions, providing a deeper strategic insight.
Reinforcement Learning with Proximal Policy Optimization:
We will implement the PPO algorithm using the Stable Baselines3 library in Python. The PPO algorithm works by optimizing a clipped surrogate objective function, which helps to balance exploration and exploitation, and ensures stable policy updates.

Why This Solution Might Work: We know that given our inexperience in this area, it may take some experimentation. But we think given how the battle can be represented mathematically, almost like like two big equations racing each other to zero HP, we will be able to leverage our state representation of the game state so that our AI can make informed decisions that take into account the complex interactions between different Pokemon moves, types, and status conditions. We think our reward shaping will be important, as we want our AI to be incentivized to learn effective battle tactics, including exploiting type advantages and predicting opponents moves. We plan to combine MCTS with PPO, with MCTS allowing us to explore long term consequences of actions, which PPO ensures efficient policy updates. 

Testing & Evaluation: 
Training Phase:
AI trained in a controlled environment against scripted opponents. Allows us to monitor learning progress and refine AI.
Testing Phase:
After training, we will test AI against scripted and human opponents, we will measure its win rate, move effectiveness, and adaptability against different opponents. 
Benchmark Model:
As a benchmark model, we can compare the performance of our AI against rule-based opponents that follow a predefined strategy. This will give us a baseline when evaluating how we can fine tune our AI. 

Libraries/Function Calls We Know We Will Use:
Showdownpy: “A client for Pokemon Showdown! for Python 3.4 and 3.5. This was written to make it easier to write bots, interact with users, moderate chat rooms, and collect data” - https://pypi.org/project/showdownpy/
PPO algorithm from StableBaselin3 Library in Python: https://stable-baselines3.readthedocs.io/en/v2.3.2/modules/ppo.html
OpenAI Gym
 To evaluate the performance of our AI agent using Monte Carlo Tree Search (MCTS) using the Upper Confidence Bounds (UCT) formula for decision-making, and compare it against the benchmark model, we plan to use the following metrics to quantify the AI’s effectiveness. 

Win Rate:
% of battles won by the AI out of a fixed number of test battles.
Win Rate = (Number of Battles Won / Total Number of Battles) x 100
Derivation: During testing, the AI will participate in a series of battles. By tracking the number of battles won by the AI, we can calculate its win rate.
Move Effectiveness:
Move effectiveness is the percentage of moves chosen by the AI that are super effective against the opponent's Pokémon. This metric measures the AI's ability to exploit type advantages, a key strategy which will increase our AI performance.
Move Effectiveness = (# of Super Effective Moves / Total # of Moves) x 100
Derivation: During each battle, we track the moves selected by the AI and check their effectiveness based on type matchups.
Adaptability:
Measured by the change in win rate when the AI faces different scripted opponents with varying strategies. This metric assesses the AI's ability to adapt to different play styles.
Adaptability = i=1n(Win Ratei+1- Win Ratein, where n = number of scripted opponents.
Derivation: By testing the AI against a series of scripted opponents, we can observe changes in win rate. A higher adaptability score indicates that the AI can effectively adjust its strategy based on the opponent's behavior.
How Data from Battles Fits the Models for Training and Testing:
 Monte Carlo Tree Search (MCTS)
State Representation: Each state st ​ includes detailed information about the game: State t ​ =(Our Array, Opponent’s Array, Current Our Pokemon, Current Opponent’s Pokemon,Turn)
Action Space: The set of possible actions at ​ the AI can take includes: 
at ​ ={Move 1,Move 2,Move 3,Move 4,Switch Pokemon}
Monte Carlo Search Tree (MCTS) Process:
Selection: Start from the root node and select child nodes using the UCT formula until a leaf node is reached:
aat= arg maxa(Q(st,a) + cln N(st)N(st,a)
where  Q(st ​,a) is the average reward for taking action a in state st​, N(st) is the visit count for state  st​, and N(st,a) is the visit count for action a in state  st.
Expand the search tree by adding a new node for each possible action. 
Simulate the game from the current state to a terminal state by selecting actions according to policy.
Evaluation: Calculate the reward for the terminal state using the evaluation function: Utility(s) = ∑Agent’s Health - ∑ Opponent’s Health
Backpropagate the reward through the tree, updating the values of the visited nodes.
Implementation in Training and Testing:
 During training the AI will use the tree to explore potential future states and inform decision-making. The data from each battle (states, actions, rewards) will be used to update the search tree.
In testing, we will evaluate the AI using the proposed metrics. By comparing these metrics against a benchmark model, we can assess our Ai’s performance.
Benchmark Model
Definition: A rule-based opponent that follows a predefined strategy, such as always choosing the highest damage move or switching to a type-advantaged Pokémon. 
Comparison: The AI's performance will be compared to this benchmark model using the evaluation metrics. Improvements in win rate, move effectiveness, and adaptability over the benchmark will demonstrate success.
"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ethics & Privacy\n",
        "The main goal of our project is to develop an AI agent using MCTS and RL techniques to make strategic decisions to maximize winning expectation of Pokémon battles. While we also seriously consider ethical and privacy aspects in our project.
Data privacy & consent: Data Privacy & Consent:
Since our project utilizes battle data from various sources, which may include sensitive information regarding the game and its users, we will ensure that all data is anonymized and handled in compliance with data privacy regulations. We will strictly adhere to data protection laws, ensuring the privacy and consent of all individuals involved in the data collection process.
Bias and Representation:
The dataset used in our project might contain inherent biases due to factors such as cultural preferences, language, and geographical distribution. We acknowledge that the initial dataset may not equally represent all types of Pokemon battles or strategies. To address this, we will actively work to minimize and mitigate such biases, striving to create a more balanced and inclusive AI model that can cater to diverse battling styles and preferences.
Generalization and Impact:
Our project dataset is limited, but we aim to develop methods to generalize the AI's performance across various battle scenarios and Pokemon types. We will take care to avoid overgeneralization, which could misrepresent certain battle strategies or Pokemon characteristics. Our goal is to ensure that the AI provides recommendations and strategies that benefit a broad spectrum of players and battle conditions.
Transparency and Accountability:
In line with ethical research practices, we commit to full transparency regarding our methodologies, data sources, and findings. We will document and share our research processes, ensuring that our work can be reviewed, reproduced, and validated by others in the community. This transparency will help us maintain accountability and foster trust in our project's outcomes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Team Expectations\n",
        "We expect each member to keep the other members updated on their progress and let us know in advance if they are unable to complete their work.

We will regularly meet at the planned times and will let the other members know if something prevents us from attending.

Each member will complete their portion of the work on time and let the other members know if the load is too much work for them.

We will respect each member and listen to each other regardless of our differences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Project Timeline\n",
        "
Meet Date
Meet Time
Completed Before Meeting
Discuss at Meeting
5/9
5 PM
Brainstorm topics/questions
Determine best way of communication, discuss and finalize project idea, discuss problem/solution, determine best days and time for meetings
5/14
3 PM
Do background research on topic
Discuss and finalize the ideal dataset that may be used to the project, begin draft of project proposal and split independent work,
5/17
5PM
Edit proposal
Discuss and agree as a group on team expectations and team calendar, go through each part of proposal together and make any final edits, finalize and submit proposal
5/20
5PM
Begin cleaning the dataset only including the battle related data
Discuss and assign group members to lead specific parts of project, discuss analysis plan
6/01
5PM
Begin programming for project
Discuss and edit code, discuss what are the final steps for the project code
6/07
5PM
Finalize results, draft results/conclusion/discussion
Discuss and split the remaining parts from the project for individual work
6/11
5PM
Review and finalize project
Turn in Final Project

"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Footnotes\n",
        "Barradas, A. Pokemon with stats. Retrieved May 17, 2024 from https://www.kaggle.com/datasets/abcsds/pokemon/data.
Nguyen Van Anh, T. Pokemon Dataset with Team Combat. Retrieved May 17, 2024 from https://www.kaggle.com/datasets/tuannguyenvananh/pokemon-dataset-with-team-combat.
Silver, D., et al. "Mastering the game of Go with deep neural networks and tree search." Nature 529.7587 (2016): 484-489.

OpenAI. "OpenAI Five: The world’s first AI to defeat a team of professionals at Dota 2." OpenAI, 2018.

Kaggle dataset on Generation 1 Pokémon: https://www.kaggle.com/abcsds/pokemon
"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
